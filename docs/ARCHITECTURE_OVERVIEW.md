# ğŸ—ï¸ SIIC - System Architecture Overview

## ğŸ“‹ Table of Contents
1. [System Overview](#system-overview)
2. [Model Architecture](#model-architecture)
3. [Data Pipeline](#data-pipeline)
4. [Training Pipeline](#training-pipeline)
5. [Inference Pipeline](#inference-pipeline)
6. [Performance Analysis](#performance-analysis)
7. [Deployment Architecture](#deployment-architecture)
8. [Technology Stack](#technology-stack)

---

## ğŸ¯ System Overview

**SIIC (Sentiment Intelligence and Insight Center)** lÃ  má»™t há»‡ thá»‘ng phÃ¢n tÃ­ch cáº£m xÃºc tiáº¿ng Viá»‡t sá»­ dá»¥ng multiple model approaches Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘i Æ°u.

### ğŸ¯ Core Objectives
- **Accuracy**: Äáº¡t >90% accuracy trÃªn UIT-VSFC dataset
- **Robustness**: Support multiple model types Ä‘á»ƒ compare performance  
- **Scalability**: Dá»… dÃ ng thÃªm models má»›i vÃ  scale horizontally
- **Maintainability**: Clear separation of concerns vÃ  modular design

---

## ğŸ¤– Model Architecture

### ğŸ† Model Performance Ranking

| Rank | Model | Type | Accuracy | F1-Score | Characteristics |
|------|-------|------|----------|----------|----------------|
| ğŸ¥‡ | **PhoBERT** | Transformer | **93.74%** | **93.44%** | Best overall, efficient training |
| ğŸ¥ˆ | **SVM** | Traditional ML | **85.61%** | **84.88%** | Surprisingly competitive |
| ğŸ¥‰ | **LSTM** | Deep Learning | **85.02%** | **85.56%** | Good but overfitting issues |
| 4ï¸âƒ£ | **Random Forest** | Traditional ML | **82.90%** | **81.93%** | Stable baseline |
| 5ï¸âƒ£ | **Logistic Regression** | Traditional ML | **82.23%** | **83.42%** | Simple baseline |

### ğŸ§  Model Details

#### 1. **PhoBERT (Transformer) ğŸ¥‡**
```python
Architecture:
â”œâ”€â”€ PhoBERT Tokenizer (Vietnamese pre-trained)
â”œâ”€â”€ PhoBERT Encoder (12 layers, 768 hidden size)
â”œâ”€â”€ Classification Head (Dropout + Linear)
â””â”€â”€ Output Layer (3 emotion classes)

Training:
- Epochs: 2 only
- Learning Rate: 2e-5
- Batch Size: 16
- No overfitting observed
```

#### 2. **LSTM (Deep Learning) ğŸ¥‰**
```python
Architecture:
â”œâ”€â”€ Embedding Layer (vocab_size=10000, embed_dim=128)
â”œâ”€â”€ Bidirectional LSTM (hidden_size=128, num_layers=2)
â”œâ”€â”€ Dropout Layer (rate=0.3)
â””â”€â”€ Dense Layer (output=3 classes)

Training:
- Epochs: 30
- Learning Rate: 0.001
- Batch Size: 32
- Issue: Overfitting (train_acc=95%, val_acc=84%)
```

#### 3. **Traditional ML Models ğŸ¥ˆ**
```python
SVM:
- Kernel: RBF
- C: 1.0
- Class Weight: Balanced
- Features: TF-IDF vectors

Random Forest:
- N Estimators: 100
- Class Weight: Balanced
- Features: TF-IDF vectors

Logistic Regression:
- C: 1.0
- Max Iter: 1000
- Class Weight: Balanced
- Features: TF-IDF vectors
```

---

## ğŸ“Š Data Pipeline

### ğŸ“¥ Data Sources
- **Primary**: UIT-VSFC Dataset (Vietnamese Sentiment Dataset)
- **Format**: CSV with text and emotion labels
- **Classes**: 3 emotions (Negative, Neutral, Positive)

### ğŸ”§ Preprocessing Pipeline
```python
Raw Text â†’ Text Cleaning â†’ Tokenization â†’ Feature Engineering
    â†“              â†“              â†“              â†“
UIT-VSFC    Remove noise   Split words   TF-IDF/Embeddings
Dataset     Special chars  Normalize     PhoBERT tokens
```

### ğŸ¯ Feature Engineering Strategies

#### 1. **For Traditional ML (TF-IDF)**
- Max features: 10,000
- N-grams: (1,2)
- Stop words: Vietnamese stop words
- Min/Max document frequency filtering

#### 2. **For LSTM (Word Embeddings)**
- Vocabulary size: 10,000
- Embedding dimension: 128
- Sequence length: 256
- Padding/Truncation strategies

#### 3. **For PhoBERT (Tokenization)**
- Pre-trained Vietnamese tokenizer
- Max length: 256
- Special tokens: [CLS], [SEP], [PAD]
- Subword tokenization

---

## ğŸš€ Training Pipeline

### ğŸ”„ Training Workflow
```python
Data Loading â†’ Preprocessing â†’ Model Training â†’ Evaluation â†’ Model Saving
     â†“              â†“              â†“              â†“            â†“
Load CSV    Clean & Split   Train models   Calculate     Save to
UIT-VSFC    Train/Val/Test  Multiple types metrics       models/
```

### ğŸ“ˆ Training Strategies

#### **Traditional ML Training**
- **Quick Training**: No epochs, fit once
- **Cross Validation**: 5-fold for robust evaluation  
- **Hyperparameter Tuning**: Grid search for optimal params
- **Feature Selection**: TF-IDF optimization

#### **LSTM Training**
- **Epochs**: 30 with early stopping
- **Optimizer**: Adam (lr=0.001)
- **Loss**: CrossEntropyLoss
- **Regularization**: Dropout (0.3), Early stopping
- **Problem**: Overfitting detected at epoch 20+

#### **PhoBERT Training**
- **Epochs**: 2 only (very efficient!)
- **Optimizer**: AdamW (lr=2e-5)
- **Scheduler**: Linear warmup + decay
- **Regularization**: Built-in dropout in transformer layers
- **Advantage**: No overfitting, fast convergence

---

## ğŸ”® Inference Pipeline

### âš¡ Real-time Prediction Flow
```python
User Input â†’ Preprocessing â†’ Model Selection â†’ Prediction â†’ Post-processing â†’ Response
    â†“             â†“              â†“              â†“              â†“              â†“
"TÃ´i ráº¥t    Text cleaning   Choose best    Run forward    Format result  JSON response
 vui hÃ´m nay"  Tokenization   model (PhoBERT) pass         Add confidence  with emotion
```

### ğŸ¯ Model Selection Strategy
1. **Default**: PhoBERT (best performance)
2. **Fast mode**: SVM (quick inference)
3. **Ensemble**: Combine multiple models for robustness

### ğŸ“Š Output Format
```json
{
  "emotion": "positive",
  "confidence": 0.94,
  "probabilities": {
    "negative": 0.02,
    "neutral": 0.04,
    "positive": 0.94
  },
  "model_used": "PhoBERT",
  "processing_time": "0.15s"
}
```

---

## ğŸ“ˆ Performance Analysis

### ğŸ¯ Key Metrics

#### **Overall Performance**
- **Best Model**: PhoBERT (93.74% accuracy)
- **Baseline vs SOTA**: 10%+ improvement over traditional ML
- **Training Efficiency**: PhoBERT trains 15x faster than LSTM
- **Inference Speed**: All models <200ms per prediction

#### **Per-Class Performance** (PhoBERT)
| Class | Precision | Recall | F1-Score | Support |
|-------|-----------|--------|----------|---------|
| Negative | 94.04% | 96.15% | 95.08% | 1,409 |
| Neutral | 64.79% | 43.81% | 52.27% | 164 |
| Positive | 95.14% | 95.85% | 95.50% | 1,590 |

#### **Training Insights**
- **PhoBERT**: Fast convergence, no overfitting
- **LSTM**: Overfitting after epoch 20
- **Traditional ML**: Quick training, competitive results
- **SVM**: Surprisingly good performance for simple model

### ğŸ” Error Analysis
- **Neutral class**: Hardest to predict (imbalanced dataset)
- **Confusion**: Some negative/positive overlap
- **Context**: Long sentences sometimes misclassified

---

## ğŸš€ Deployment Architecture

### ğŸ—ï¸ System Components

#### **Frontend Layer**
- **Web Dashboard**: Streamlit/Flask for visualization
- **REST API**: FastAPI for programmatic access
- **CLI Tools**: Python scripts for batch processing

#### **Application Layer**
- **Model Controller**: Model selection and loading
- **Emotion Predictor**: Real-time inference service
- **Model Trainer**: Training pipeline orchestration
- **Performance Evaluator**: Metrics computation and analysis

#### **Model Service Layer**
- **Model Loader**: Dynamic model loading and caching
- **Text Preprocessor**: Consistent text preprocessing
- **Result Processor**: Output formatting and confidence calculation

#### **Data Layer**
- **Data Loader**: Multi-format data ingestion
- **Data Validator**: Schema validation and quality checks
- **Feature Engineering**: Consistent feature extraction

#### **Storage Layer**
- **Model Storage**: Versioned model artifacts (models/)
- **Data Storage**: Raw and processed datasets (data/)
- **Results Storage**: Evaluation results and plots (results/)
- **Configuration**: Hyperparameters and settings (config/)

### ğŸ³ Infrastructure

#### **Containerization**
```dockerfile
# Multi-stage Docker build
Stage 1: Base dependencies (Python, PyTorch)
Stage 2: Model training environment
Stage 3: Production inference server
```

#### **Load Balancing**
- **Nginx**: Frontend proxy and load balancer
- **Multiple Workers**: Gunicorn/uWSGI for parallel processing
- **Model Caching**: Redis for model artifact caching

#### **Monitoring**
- **Metrics**: Prometheus for system metrics
- **Logging**: Structured logging with ELK stack
- **Alerting**: Model performance degradation alerts

---

## ğŸ’» Technology Stack

### ğŸ”§ Core Dependencies

#### **Machine Learning**
- **PyTorch**: Deep learning framework (LSTM, PhoBERT)
- **Transformers**: Hugging Face library (PhoBERT)
- **Scikit-learn**: Traditional ML algorithms
- **Pandas/NumPy**: Data manipulation and numerical computing

#### **NLP Specific**
- **PhoBERT**: Vietnamese pre-trained transformer
- **Tokenizers**: Fast tokenization libraries
- **NLTK/spaCy**: Text preprocessing utilities

#### **Web Framework**
- **FastAPI**: High-performance API framework
- **Streamlit**: Rapid dashboard development
- **Flask**: Lightweight web framework option

#### **Data & Storage**
- **CSV/JSON**: Data serialization formats
- **Pickle**: Model serialization
- **SQLite/PostgreSQL**: Optional database storage

#### **DevOps & Deployment**
- **Docker**: Containerization
- **Nginx**: Web server and reverse proxy
- **Git**: Version control
- **pytest**: Testing framework

### ğŸ“¦ Project Structure
```
SIIC/
â”œâ”€â”€ ğŸ“ siic/                 # Core package
â”‚   â”œâ”€â”€ ğŸ“ data/            # Data handling modules
â”‚   â”œâ”€â”€ ğŸ“ models/          # Model implementations
â”‚   â”œâ”€â”€ ğŸ“ training/        # Training pipelines
â”‚   â”œâ”€â”€ ğŸ“ evaluation/      # Evaluation tools
â”‚   â””â”€â”€ ğŸ“ utils/           # Utility functions
â”œâ”€â”€ ğŸ“ models/              # Saved model artifacts
â”œâ”€â”€ ğŸ“ data/                # Raw and processed data
â”œâ”€â”€ ğŸ“ results/             # Evaluation results
â”œâ”€â”€ ğŸ“ config/              # Configuration files
â”œâ”€â”€ ğŸ“ tests/               # Unit tests
â”œâ”€â”€ ğŸ“ docs/                # Documentation
â”œâ”€â”€ ğŸ“ api/                 # API server
â”œâ”€â”€ ğŸ“ dashboard/           # Web dashboard
â””â”€â”€ ğŸ“ scripts/             # Utility scripts
```

---

## ğŸ¯ Future Improvements

### ğŸš€ Model Enhancements
1. **Ensemble Methods**: Combine multiple models for better accuracy
2. **Model Distillation**: Create lightweight models from PhoBERT
3. **Active Learning**: Iterative improvement with human feedback
4. **Multi-label Classification**: Support multiple emotions per text

### ğŸ”§ System Improvements
1. **Real-time Training**: Online learning capabilities
2. **A/B Testing**: Model comparison in production
3. **Auto-scaling**: Dynamic resource allocation
4. **Edge Deployment**: Mobile/embedded inference

### ğŸ“Š Data Improvements
1. **Data Augmentation**: Synthetic data generation
2. **Domain Adaptation**: Support for different text domains
3. **Multilingual**: Support for other languages
4. **Streaming Data**: Real-time data ingestion

---

## ğŸ“ Contact & Support

For questions about the architecture or implementation details:

- **Documentation**: `/docs` directory
- **Issues**: GitHub issues tracker
- **API Reference**: `/api/docs` endpoint
- **Performance Metrics**: `/results` directory

---

*Generated by SIIC Architecture Documentation System*
*Last Updated: January 2025* 