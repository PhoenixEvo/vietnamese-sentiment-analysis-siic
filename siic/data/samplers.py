"""
Create large sample data that mimics UIT-VSFC dataset structure
"""
import pandas as pd
import numpy as np
import random
import os
import sys

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)

from siic.utils.config import RAW_DATA_DIR, PROCESSED_DATA_DIR

def create_large_vietnamese_sample():
    """Create large sample Vietnamese feedback data similar to UIT-VSFC"""
    
    # Templates for different emotions
    templates = {
        'happy': [
            "Th·∫ßy/C√¥ {} r·∫•t t·∫≠n t√¢m v√† nhi·ªát t√¨nh",
            "M√¥n h·ªçc {} r·∫•t hay v√† b·ªï √≠ch", 
            "T√¥i r·∫•t h√†i l√≤ng v·ªõi {}",
            "Ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y {} tuy·ªát v·ªùi",
            "C·∫£m th·∫•y vui v·∫ª khi h·ªçc {}",
            "ƒê∆∞·ª£c h·ªçc nhi·ªÅu ki·∫øn th·ª©c t·ª´ {}",
            "Th·∫ßy/C√¥ {} gi·∫£ng b√†i r·∫•t d·ªÖ hi·ªÉu",
            "M√¥n {} gi√∫p t√¥i hi·ªÉu bi·∫øt nhi·ªÅu h∆°n",
            "R·∫•t th√≠ch c√°ch {} c·ªßa th·∫ßy/c√¥",
            "B√†i gi·∫£ng {} r·∫•t sinh ƒë·ªông v√† h·∫•p d·∫´n"
        ],
        'sad': [
            "C·∫£m th·∫•y bu·ªìn v√¨ {} kh√¥ng nh∆∞ mong ƒë·ª£i",
            "Th·∫•t v·ªçng v·ªõi {}",
            "M√¥n h·ªçc {} kh√≥ hi·ªÉu qu√°",
            "Kh√¥ng theo k·ªãp {} ƒë∆∞·ª£c",
            "C·∫£m th·∫•y n·∫£n l√≤ng v·ªõi {}",
            "Stress qu√° v√¨ {}",
            "Kh√≥ khƒÉn trong vi·ªác h·ªçc {}",
            "Kh√¥ng hi·ªÉu b√†i gi·∫£ng {}",
            "C√¥ ƒë∆°n khi h·ªçc {}",
            "M·ªát m·ªèi v·ªõi {} n√†y"
        ],
        'angry': [
            "T·ª©c gi·∫≠n v·ªõi {} k√©m ch·∫•t l∆∞·ª£ng",
            "B·ª±c m√¨nh v√¨ {} kh√¥ng chuy√™n nghi·ªáp", 
            "Ph·∫´n n·ªô v·ªõi c√°ch {} c·ªßa th·∫ßy/c√¥",
            "Kh√≥ ch·ªãu v·ªõi {} n√†y",
            "B·ª©c x√∫c v√¨ {} kh√¥ng r√µ r√†ng",
            "ƒêi√™n ti·∫øt v·ªõi th√°i ƒë·ªô {}",
            "C√°u g·∫Øt v√¨ {} t·ªá",
            "Ch√°n gh√©t {} kh√¥ng hi·ªáu qu·∫£",
            "N·ªïi gi·∫≠n v·ªõi {} k√©m",
            "B·ª±c b·ªôi v√¨ {} thi·∫øu chuy√™n nghi·ªáp"
        ],
        'neutral': [
            "M√¥n {} b√¨nh th∆∞·ªùng th√¥i",
            "C≈©ng ƒë∆∞·ª£c, {} t·∫°m ·ªïn",
            "Kh√¥ng c√≥ √Ω ki·∫øn g√¨ v·ªÅ {}",
            "Nh∆∞ v·∫≠y c≈©ng ƒë∆∞·ª£c v·ªõi {}",
            "B√¨nh th∆∞·ªùng, {} kh√¥ng c√≥ g√¨ ƒë·∫∑c bi·ªát",
            "·ªîn, {} c≈©ng t·∫°m ƒë∆∞·ª£c",
            "Kh√¥ng t·ªá l·∫Øm v·ªõi {}",
            "Trung b√¨nh v·ªõi {}",
            "B√¨nh th∆∞·ªùng nh∆∞ m·ªçi khi v·ªõi {}",
            "C≈©ng ƒë∆∞·ª£c th√¥i v·ªõi {}"
        ]
    }
    
    # Subjects and topics for variation
    subjects = [
        "th·∫ßy Nguy·ªÖn", "c√¥ Tr·∫ßn", "gi·∫£ng vi√™n", "th·∫ßy/c√¥", "m√¥n To√°n",
        "m√¥n L√Ω", "m√¥n H√≥a", "m√¥n VƒÉn", "m√¥n Anh", "b√†i gi·∫£ng", 
        "ph√≤ng th·ª±c h√†nh", "thi·∫øt b·ªã", "t√†i li·ªáu", "ph∆∞∆°ng ph√°p gi·∫£ng d·∫°y",
        "c√°ch tr√¨nh b√†y", "n·ªôi dung b√†i h·ªçc", "b√†i t·∫≠p", "ƒë·ªÅ thi",
        "ch∆∞∆°ng tr√¨nh h·ªçc", "th·ªùi gian h·ªçc"
    ]
    
    # Generate samples
    all_samples = []
    target_samples = 1000  # Create 1000 samples
    
    for emotion, template_list in templates.items():
        samples_per_emotion = target_samples // 4  # 250 per emotion
        
        for i in range(samples_per_emotion):
            # Random template and subject
            template = random.choice(template_list)
            subject = random.choice(subjects)
            
            # Generate text
            text = template.format(subject)
            
            # Add some variation
            if random.random() < 0.3:  # 30% chance to add prefix
                prefixes = ["Th·ª±c s·ª±", "C√° nh√¢n t√¥i nghƒ©", "Theo √Ω ki·∫øn c·ªßa t√¥i", "Nh√¨n chung"]
                text = f"{random.choice(prefixes)}, {text.lower()}"
            
            if random.random() < 0.2:  # 20% chance to add suffix  
                suffixes = ["·∫°", "!", ".", " l·∫Øm", " qu√°"]
                text = text + random.choice(suffixes)
            
            # Convert emotion to sentiment (like UIT-VSFC)
            if emotion == 'happy':
                sentiment = 2  # positive
            elif emotion == 'neutral':
                sentiment = 1  # neutral  
            else:  # sad or angry
                sentiment = 0  # negative
            
            # Assign split
            rand = random.random()
            if rand < 0.7:
                split = 'train'
            elif rand < 0.85:
                split = 'validation'
            else:
                split = 'test'
            
            all_samples.append({
                'text': text,
                'sentiment': sentiment,
                'emotion': emotion,
                'split': split
            })
    
    # Shuffle samples
    random.shuffle(all_samples)
    
    return pd.DataFrame(all_samples)

def create_realistic_feedback_data():
    """Create more realistic student feedback data"""
    
    feedback_examples = [
        # Happy feedbacks
        ("Th·∫ßy gi·∫£ng b√†i r·∫•t hay v√† d·ªÖ hi·ªÉu, t√¥i h·ªçc ƒë∆∞·ª£c nhi·ªÅu ki·∫øn th·ª©c b·ªï √≠ch", "happy"),
        ("C√¥ r·∫•t t·∫≠n t√¢m v√† nhi·ªát t√¨nh h∆∞·ªõng d·∫´n, t√¥i c·∫£m th·∫•y r·∫•t h√†i l√≤ng", "happy"),
        ("M√¥n h·ªçc n√†y th·∫≠t s·ª± th√∫ v·ªã v√† h·ªØu √≠ch cho chuy√™n ng√†nh c·ªßa t√¥i", "happy"),
        ("Ph√≤ng h·ªçc v√† thi·∫øt b·ªã r·∫•t t·ªët, h·ªó tr·ª£ t·ªët cho vi·ªác h·ªçc", "happy"),
        ("Th·∫ßy lu√¥n ƒëi d·∫°y ƒë√∫ng gi·ªù v√† chu·∫©n b·ªã b√†i k·ªπ l∆∞·ª°ng", "happy"),
        
        # Sad feedbacks  
        ("M√¥n h·ªçc kh√≥ qu√°, t√¥i kh√¥ng theo k·ªãp ƒë∆∞·ª£c", "sad"),
        ("C·∫£m th·∫•y stress v√¨ √°p l·ª±c h·ªçc t·∫≠p qu√° l·ªõn", "sad"),
        ("Bu·ªìn v√¨ kh√¥ng hi·ªÉu b√†i gi·∫£ng c·ªßa th·∫ßy", "sad"),
        ("Th·∫•t v·ªçng v√¨ k·∫øt qu·∫£ h·ªçc t·∫≠p kh√¥ng nh∆∞ mong mu·ªën", "sad"),
        ("C·∫£m th·∫•y c√¥ ƒë∆°n khi h·ªçc online", "sad"),
        
        # Angry feedbacks
        ("Th·∫ßy d·∫°y kh√¥ng r√µ r√†ng v√† th√°i ƒë·ªô kh√¥ng t·ªët", "angry"),
        ("T·ª©c gi·∫≠n v√¨ thi·∫øt b·ªã trong ph√≤ng h·ªçc b·ªã h·ªèng", "angry"),
        ("B·ª±c m√¨nh v√¨ l·ªãch h·ªçc thay ƒë·ªïi li√™n t·ª•c", "angry"),
        ("Ph·∫´n n·ªô v·ªõi c√°ch ch·∫•m ƒëi·ªÉm kh√¥ng c√¥ng b·∫±ng", "angry"),
        ("Kh√≥ ch·ªãu v√¨ ph√≤ng h·ªçc qu√° ·ªìn √†o", "angry"),
        
        # Neutral feedbacks
        ("M√¥n h·ªçc b√¨nh th∆∞·ªùng, kh√¥ng c√≥ g√¨ ƒë·∫∑c bi·ªát", "neutral"),
        ("C≈©ng ƒë∆∞·ª£c th√¥i, t·∫°m ·ªïn", "neutral"),
        ("Kh√¥ng c√≥ √Ω ki·∫øn g√¨ ƒë·∫∑c bi·ªát", "neutral"),
        ("Nh∆∞ v·∫≠y c≈©ng ƒë∆∞·ª£c", "neutral"),
        ("B√¨nh th∆∞·ªùng nh∆∞ m·ªçi khi", "neutral"),
    ]
    
    # Expand with variations
    expanded_data = []
    
    for base_text, emotion in feedback_examples:
        # Add base example
        expanded_data.append((base_text, emotion))
        
        # Create variations
        for i in range(19):  # Create 19 more variations (total 20 per base)
            # Simple variations
            variations = [
                base_text + ".",
                base_text + " ·∫°.",
                base_text + " l·∫Øm.",
                "Theo t√¥i, " + base_text.lower(),
                "C√° nh√¢n t√¥i nghƒ© " + base_text.lower(),
                base_text + " qu√°.",
                "Th·ª±c s·ª± " + base_text.lower(),
                base_text.replace("t√¥i", "em"),
                base_text.replace("Th·∫ßy", "Gi√°o vi√™n"),
                base_text.replace("C√¥", "Th·∫ßy c√¥"),
            ]
            
            if i < len(variations):
                expanded_data.append((variations[i], emotion))
            else:
                # Simple repetition with minor changes
                expanded_data.append((base_text, emotion))
    
    # Convert to DataFrame
    all_samples = []
    for text, emotion in expanded_data:
        # Convert emotion to sentiment
        if emotion == 'happy':
            sentiment = 2
        elif emotion == 'neutral':
            sentiment = 1
        else:
            sentiment = 0
        
        # Assign split
        rand = random.random()
        if rand < 0.7:
            split = 'train'
        elif rand < 0.85:
            split = 'validation'
        else:
            split = 'test'
        
        all_samples.append({
            'text': text,
            'sentiment': sentiment, 
            'emotion': emotion,
            'split': split
        })
    
    return pd.DataFrame(all_samples)

def save_large_sample_data(df, filename="large_sample_data"):
    """Save large sample data in UIT-VSFC format"""
    # Ensure directories exist
    os.makedirs(RAW_DATA_DIR, exist_ok=True)
    
    # Save main file
    main_path = os.path.join(RAW_DATA_DIR, f'{filename}.csv')
    df.to_csv(main_path, index=False)
    print(f"üíæ Large sample data saved to: {main_path}")
    
    # Save splits separately
    for split in ['train', 'validation', 'test']:
        split_df = df[df['split'] == split].copy()
        split_path = os.path.join(RAW_DATA_DIR, f'{filename}_{split}.csv')
        split_df.to_csv(split_path, index=False)
        print(f"üíæ {split.title()} split saved to: {split_path}")
    
    return main_path

def analyze_large_sample(df):
    """Analyze the generated large sample data"""
    print("\n" + "="*50)
    print("üìä LARGE SAMPLE DATA ANALYSIS")
    print("="*50)
    
    print(f"Total samples: {len(df):,}")
    print(f"Unique texts: {df['text'].nunique():,}")
    print(f"Average text length: {df['text'].str.len().mean():.1f} characters")
    
    print(f"\nEmotion distribution:")
    emotion_counts = df['emotion'].value_counts()
    for emotion, count in emotion_counts.items():
        percentage = count / len(df) * 100
        print(f"  {emotion}: {count:,} ({percentage:.1f}%)")
    
    print(f"\nSplit distribution:")
    split_counts = df['split'].value_counts()
    for split, count in split_counts.items():
        percentage = count / len(df) * 100
        print(f"  {split}: {count:,} ({percentage:.1f}%)")

def main():
    """Main function to create large sample data"""
    print("Creating Large Sample Data (UIT-VSFC Alternative)")
    print("="*60)
    
    # Set seed for reproducibility
    random.seed(42)
    np.random.seed(42)
    
    print("üîÑ Generating template-based samples...")
    df1 = create_large_vietnamese_sample()
    print(f"   Generated {len(df1)} template-based samples")
    
    print("üîÑ Generating realistic feedback samples...")
    df2 = create_realistic_feedback_data()
    print(f"   Generated {len(df2)} realistic samples")
    
    # Combine datasets
    combined_df = pd.concat([df1, df2], ignore_index=True)
    
    # Shuffle
    combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    print(f"\n‚úÖ Total combined samples: {len(combined_df)}")
    
    # Analyze data
    analyze_large_sample(combined_df)
    
    # Save data  
    data_path = save_large_sample_data(combined_df, "uit_vsfc_large_sample")
    
    print("\nüéâ Large sample data created successfully!")
    print(f"üìÅ Data saved as: {data_path}")
    print("\nThis dataset mimics UIT-VSFC structure with:")
    print("- Similar emotion categories")
    print("- Vietnamese student feedback style")
    print("- Train/validation/test splits")
    print("- Sentiment labels (0=negative, 1=neutral, 2=positive)")
    
    print("\nNext steps:")
    print("1. Run preprocessing: python src/data_processing/preprocess.py")
    print("2. Train models: python src/models/baseline_models.py")

if __name__ == "__main__":
    main() 